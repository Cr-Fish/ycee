from transformers import AutoTokenizer
from bigdl.llm.transformers import AutoModel
from streamlit.components.v1 import html
import streamlit as st
import pandas as pd
import numpy as np
import torch

# åˆ›å»ºStreamlitåº”ç”¨
st.set_page_config(
    page_title="YCEE-å®šåˆ¶ä½ çš„æ—…æ¸¸æ”»ç•¥",
    page_icon=":robot:"
)

# åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    model_path = "THUDM/chatglm2-6b"
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModel.from_pretrained(model_path, load_in_4bit=True, trust_remote_code=True)
    model = model.eval()
    return tokenizer, model

tokenizer, model = get_model()
st.markdown(
    """
    <style>
    #root > div:nth-child(1) > div.withScreencast > div > div > div > section.main.css-uf99v8.e1g8pov65 > div.block-container.css-1y4p8pa.e1g8pov64 {
        font-size: 20px;
        text-align: center;
        font-family: 'å¾—æ„é»‘', serif;
        color: orange;
    }
    #ycee > div > span {
        font-size: 60px;
        color: #9c3400;
        font-family: Times New Roman; /* è®¾ç½®æ–‡æœ¬çš„å­—ä½“ */
    }
    #root > div:nth-child(1) > div.withScreencast > div > div > div > section.main.css-uf99v8.e1g8pov65 > div.block-container.css-1y4p8pa.e1g8pov64 > div:nth-child(1) > div > div:nth-child(4) > div > div > p {
        font-size: 30px;
        color: orange;
        font-family: 'å¾—æ„é»‘'; /* è®¾ç½®æ–‡æœ¬çš„å­—ä½“ */
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Streamlitç»„ä»¶
st.title("YCEE")
st.write("ğŸ›«åœ¨çº¿å®šåˆ¶å±äºä½ çš„æ—…æ¸¸æ”»ç•¥")
df = pd.DataFrame(
    columns=['lat', 'lon']
)
st.map(df)
max_length = st.sidebar.slider(
    'max_length', 0, 32768, 8192, step=1
)
top_p = st.sidebar.slider(
    'top_p', 0.0, 1.0, 0.8, step=0.01
)
temperature = st.sidebar.slider(
    'temperature', 0.0, 1.0, 0.8, step=0.01
)
if 'history' not in st.session_state:
    st.session_state.history = []

# if 'past_key_values' not in st.session_state:
#     st.session_state.past_key_values = None

for i, (query, response) in enumerate(st.session_state.history):
    with st.chat_message(name="user", avatar="user"):
        st.markdown(query)
    with st.chat_message(name="assistant", avatar="assistant"):
        st.markdown(response)
with st.chat_message(name="user", avatar="user"):
    input_placeholder = st.empty()
with st.chat_message(name="assistant", avatar="assistant"):
    message_placeholder = st.empty()



# å®šä¹‰prompt
prompt = """ç”Ÿæˆä»»åŠ¡ï¼šå°†ç”¨æˆ·è¾“å…¥çš„æ—…æ¸¸åå¥½ç”Ÿæˆæ—…æ¸¸è·¯çº¿ä¿¡æ¯ã€‚

ä¸‹é¢æ˜¯ä¸€äº›èŒƒä¾‹:

å¤§ç¾æ–°ç–†ï¼Œäº”å¤©ç®€æ¸¸ï¼Œå¥½çœ‹å¥½åƒå¥½ç•™æ‹ã€‚2023-10-02 å‡ºå‘,å…±1å¤©ï¼Œäº«å—ç¾é£Ÿï¼  ->    åŸå¸‚ï¼šé˜œåº·>åšä¹>ä¼ŠçŠ>ä¹Œé²æœ¨é½  é€”ç»ï¼šå¤©å±±å¤©æ± >èµ›é‡Œæœ¨æ¹–>é‚£æ‹‰æè‰åŸ  äººå‡2400å…ƒ
ä¸è¾œè´Ÿæ¯ä¸€ä¸ªå¤å¤©ï½10æ—¥è‡ªé©¾æ¸¸  å››å·é›…å®‰â¡ï¸æµ·å—ã€‚2023-05-01 å‡ºå‘,å…±10å¤©,é—ºèœœ,ç©·æ¸¸,ç¾é£Ÿï¼Œäº”ä¸€      ->    åŸå¸‚ï¼šé™µæ°´>ä¸‰äºš   é€”ç»ï¼šé™µæ°´é˜¿ç½—å“ˆåˆ«å¢…(æ¸…æ°´æ¹¾å¤§é“8å·)>å‘†å‘†å²›>åæµ·æ‘>ç§¦éŸµé¤å…Â·ä¸‰äºšçº¢æ ‘æ—åº¦å‡ä¸–ç•Œ   äººå‡4000å…ƒ
å»é•¿æ˜¥ï¼Œåº¦ä¸€ä¸ª25Â°çš„æ¸…çˆ½å¤æ—¥ã€‚2023-06-01 å‡ºå‘,å…±1å¤©ï¼Œç¾é£Ÿï¼Œå¤å­£      ->    åŸå¸‚ï¼šé•¿æ˜¥   é€”ç»ï¼šä¼ªæ»¡çš‡å®«åšç‰©é™¢>ä¸€æ±½çº¢æ——æ–‡åŒ–å±•é¦†>é•¿æ˜¥æ°´æ–‡åŒ–ç”Ÿæ€å›­>é•¿å½±æ—§å€åšç‰©é¦†>é•¿æ˜¥ç”µå½±åˆ¶ç‰‡å‚
ã€ä¸Šæµ·ã€‘æ¾æ±Ÿï¸±é­”éƒ½ä¹‹æ ¹ï¼Œäº‘é—´é£æƒ…ï¼Œå¸¦æ¥æ€æ ·çš„æƒŠå–œã€‚2023-05-08 å‡ºå‘,å…±4å¤©,ç‹¬è‡ªä¸€äºº,æ·±åº¦æ¸¸      ->    åŸå¸‚ï¼šä¸Šæµ·      é€”ç»ï¼šä¸ƒå®è€è¡—>é†‰ç™½æ± >æ›²æ°´å›­>ç§‹éœåœƒ>è±«å›­>å¤çŒ—å›­>ä»“åŸå†å²æ–‡åŒ–é£è²ŒåŒº>æ¾æ±Ÿå¸ƒå±•ç¤ºé¦†    äººå‡1280å…ƒ

è¯·å¯¹ä¸‹è¿°è¦æ±‚ç”Ÿæˆæ—…æ¸¸è·¯çº¿ä¿¡æ¯ï¼š

xxxxxx ->
"""

user_input = st.text_area(label="ç”¨æˆ·å‘½ä»¤è¾“å…¥", 
                          height=100, 
                          placeholder=prompt)

if st.button("ç”Ÿæˆæ—…æ¸¸è·¯çº¿ä¿¡æ¯"):
    input_placeholder.markdown(user_input)
    history= st.session_state.history
    question = prompt.replace('xxxxxx', user_input)
    responses = []

    with torch.inference_mode():
        for response, history in model.stream_chat(tokenizer, question, history=[]):
            # responses.append(response)
            message_placeholder.markdown(response)

    # for response in responses:
    #     st.write(response)
    st.session_state.history = history
